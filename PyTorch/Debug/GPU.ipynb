{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3FoZ6+GUIk5mIQ2Zzzx7W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AteBraak/ML-Examples/blob/main/PyTorch/Debug/GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR_cQEngRCEd"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrBtHJ2NQ8Jk",
        "outputId": "e360288f-c60e-462b-be2d-7080f02d2b75"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "if use_gpu: \n",
        "    print(\"Using the following accelerator type: {}\".format(torch.cuda.get_device_name(0)))\n",
        "    gpu_mem_in_gb = torch.cuda.get_device_properties(0).total_memory * 1e-9\n",
        "    print('The GPU memory is {:.2f} GB'.format(gpu_mem_in_gb))\n",
        "else:\n",
        "    print(\"No GPU/TPU found, try changing the runtime type if using colab.\")\n",
        "\n",
        "print(\"Torch version: \", torch.__version__)\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the following accelerator type: Tesla K80\n",
            "The GPU memory is 12.00 GB\n",
            "Torch version:  1.8.1+cu101\n",
            "GPU Available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dM2W3DUBSRt0",
        "outputId": "5e6f8e7b-163c-438a-f9da-709ef8d5073e"
      },
      "source": [
        "if use_gpu: \n",
        "  !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "  !pip install gputil\n",
        "  !pip install psutil\n",
        "  !pip install humanize\n",
        "  import psutil\n",
        "  import humanize\n",
        "  import os\n",
        "  import GPUtil as GPU\n",
        "  GPUs = GPU.getGPUs()\n",
        "  # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "  gpu = GPUs[0]\n",
        "  def printm():\n",
        "   process = psutil.Process(os.getpid())\n",
        "   print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "   print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "  printm() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=6e09afa7974c0716b5802a90a671e2e0ed24cdfc3fc3484e7512a341c30950cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 271.7 MB\n",
            "GPU RAM Free: 11438MB | Used: 3MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N42gX9OPSt4r"
      },
      "source": [
        "#!kill -9 -1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}